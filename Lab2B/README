NAME: ARNAV GARG
UID: 304911796
EMAIL: arnavgrg@ucla.edu
SLIPDAYS: 0

Files:

    - SortedList.h && SortedList.c:
            Together, they contain function definitions and function implementations to 
            insert, delete or search for an element in a doubly linked list, as well as 
            calculate the length of sorted linked list.

    - lab2_list.c:
            This is a C program that tests a shared sorted linked list. It has capabilities
            to multithread to show the existence of race conditions, as well as the 
            ability to demonstrate scheduling and synchronization mechanisms to get rid 
            of any existing race conditions.

    - lab2b_list.csv
            This contains the output of all the tests in the Makefile that use lab2_list.c 
            to display the existence of race conditions, as well as the ability to use 
            synchronization methods to stop race conditions from occuring. 

    - lab2b_plot.gp:
            This is a graphing module that uses gnuplot to create 5 different graphs for 
            output in lab2_list.csv, namely, lab2_list-1.png, lab2_list-2.png, lab2_list-3.png, 
            and  lab2_list-4.png that contain different charts about usage/timing statistics.
    
    - lab2b_1.png:
            A plot/chart that displays the average time per (single threaded) unprotected 
            operation vs. number of iterations (illustrating the correction of the per-operation 
            cost for the list length).

    - lab2b_2.png:
            A plot/chart that displays the number of threads and iterations required to 
            generate a failure (with and without yields).
            
    - lab2b_3.png: 
            A plot/chart that displays the number of iterations that can run (protected) without 
            failure.
            
    - lab2b_4.png:
            A plot/chart that displays the length-adjusted cost per operation vs the number of 
            threads for the various synchronization options.

    - lab2b_5.png:
            This is a graphing module that uses gnuplot to create 5 different graphs for 
            output in lab2_add.csv, namely, lab2_add-1.png, lab2_add-2.png, lab2_add-3.png, 
            lab2_add-4.png and lab2_add-5.png that contain different charts about 
            usage/timing statistics.

    - README:
            The README file contains a description of the all the files in the compressed
            tarball, as well as answers to questions for Part-1 and Part-2 of Lab2A. 
    
    - Makefile:
            The Makefile supports build, tests, graphs, clean and dist. Build acts like 
            make default, tests run tests for lab2_add.c and lab2_list.c, graphs takes 
            the .csv files produced by tests and produces 9 different charts. Clean 
            removes all txt, object, png and .tar.gz files. Dist creates the compressed 
            tarball.

    - profile.out:

    - tests.sh:
            
            

Questions:
    
    QUESTION 2.3.1 - CPU time in the basic list implementation:
        A. Where do you believe most of the CPU time is spent in the 1 and 2-thread list 
           tests ?
        B. Why do you believe these to be the most expensive parts of the code?
        C. Where do you believe most of the CPU time is being spent in the high-thread 
           spin-lock tests?
        D. Where do you believe most of the CPU time is being spent in the high-thread 
           mutex tests?
    ANSWER 2.3.1:
        A. Most of the time is spent in the executing each list operations (insert, lookup, 
           lengths and deletes.) because it will probably be a lot more expensive than checking 
           if there is currently a lock, which should not be computationally expensive. 
        B. This seems like a reasonable guess because one CPU can run upto two threads,
           which means that there is a limited amount of overhead cost from context-switches.
           Locks are just simple checks and therefore not computationally expensive. Additionally,
           with one threads, there is no contentions, while in the case of two threads, there are
           very few instances when the threads compete.
           Therefore, most of the time must be spent actually executing list operations.
        C. Most of the time is probably being spent during "spinning", especially while the lock is 
           obtained since the threads keep using CPU cycles/time to check if the lock is 
           free. This is a costly and continuous process.
        D. Most of the time in high-thread mutext tests is spent in the above mentioned list
           operations. This is because of the way mutex locks work. The threads will not 
           spend CPU time/clock cycles trying to check/obtain the clock, and will instead be
           put to sleep while the section is locked.
        

    QUESTION 2.3.2 - Execution Profiling:
        A. Where (what lines of code) are consuming most of the CPU time when the spin-lock 
                version of the list exerciser is run with a large number of threads?
        B. Why does this operation become so expensive with large numbers of threads?
    ANSWER 2.3.2:
        A. From the output seen in profile.out file, it is clear that the program spends 
           most of its time in the spin-lock call while loop.
                while (__sync_lock_test_and_set(&splock, 1));
           The code for lab2_list.c was updated after the initial run so that pthread methods are 
           further split into helper functions that carry out different list operations. The initial
           profile for --list=start_routine suggests that majority of the time is spent within the 
           l_insert and l_delete methods. 
           I added 2 more calls to pprof in my Makefile to profile both of these functions:
                1. -pprof --list=l_insert ./lab2_list ./raw.gperf >> profile.out 2>/dev/null
                2. -pprof --list=l_delete ./lab2_list ./raw.gperf >> profile.out 2>/dev/null
           The profile for both of these function calls are also appended to profile.out.
           The data from the profile for both of these functions suggests that majority of the time 
           is spent on the while (__sync_lock_test_and_set(&l->s_lock, 1)); method call, which 
           is consistent with the older/earlier implementation of Lab2B seen at the beginning of the 
           response for this subpart of question 2.3.2.
        B. Given a larger number of threads, there are more instances when the threads 
           compete with each other for the critical section. In other words, there is a higher
           chance that the lock is unavailable. Therefore, the remaining threads will 
           continue to spin and use CPU cycles/time until the lock is available. Each thread
           will, on average, have to wait longer for its turn once the lock becomes available.


    QUESTION 2.3.3 - Mutex Wait Time:
        Look at the average time per operation (vs. # threads) and the average wait-for-mutex 
        time (vs. #threads).
	A. Why does the average lock-wait time rise so dramatically with the number of contending 
           threads?
	B. Why does the completion time per operation rise (less dramatically) with the number of 
           contending threads?
	C. How is it possible for the wait time per operation to go up faster (or higher) than the 
           completion time per operation?
    ANSWER 2.3.3:
        A. Since more threads are competing for the critical section, there is a higher likelihood 
	   that the section is currently locked. Therefore, each thread will have to wait for longer, 
	   on average, for its turn. Additionally, when the lock does become available, there is a 
	   larger chance that threads line up in a queue to wait for their turn to execute the 
	   critical section. This also increases the average lock-wait time.
	B. Since threads spend more time waiting for locks to free up, the overall completion time
	  increases. In other words, since there are more chances of contention, there is an 
	   increase in overall completion time. However, this increases less sharply than the line for
	  lock-wait time because the time per operation (insert, lookup, delete) is constant. 
	C. Total completion time only factors in time for the parent thread and accounts for the
	    wall time for the entire operation. However, each thread measures its own wait-time during 
	   lock contentions. This wait time keeps increasing for the total number of lock contentions for 
	   each thread. Therefore, since there are multiple threads contributing towards wait time per 
	   operation, this value can be greater than the completion time per operation. 

    
    QUESTION 2.3.4 - Performance of Partitioned Lists
	A. Explain the change in performance of the synchronized methods as a function of the number 
           of lists.
	B. Should the throughput continue increasing as the number of lists is further increased? If 
           not, explain why not.
	C. It seems reasonable to suggest the throughput of an N-way partitioned list should be 
           equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear 
           to be true in the above curves? If not, explain why not.
    ANSWER 2.3.4:
    	A. The performance of the synchronized methods increases with an increase in the number of lists. 
	   This is because as we increase the number of lists, each list will contain fewer elements. This 
	   ensures that the possibilities of contention for a lock descreases in comparison to the case with
	   a single list. 
	B. The graphs show that the lines for larger number of lists tend to level off as the number of 
	   threads increase. Once the number of lists equals the total number of threads, there will be little
	   to almost no chances of lock contention. Therefore, creating additional lists will no longer 
	   impact/improve performance. We can also conclude that throughput is therefore directly dependent on 
	   the CPU and the maximum number of threads that can run on a CPU. If number of lists > 2 * number of 
	   threads, then there is no significant gain in performance.
	C. No, this does not appear to be true. This is because partitioning a list results in each sublist 
	   being significantly smaller. Therefore, less time is spent in the critical section/locked section
	   because there are fewer elements to perform list operations on. Therefore, the possibilitiy of 
	   contention is decreased since each thread will probably finish executing a critical section by the
	   time the next thread wants to get into the critical section. Reducing the number of threads 
	   also decreases the posibility of contention and wait time per thread. 


External References:
        1. https://sourceware.org/binutils/docs/gprof/ 
        2. http://www.cse.yorku.ca/~oz/hash.html 
        3. https://en.wikibooks.org/wiki/Algorithm_Implementation/Hashing 
