NAME: ARNAV GARG
UID: 304911796
EMAIL: arnavgrg@ucla.edu
SLIPDAYS: 2

Files:

    - lab2_add.c:
            This is a C program with the capability of multithreading that implements an 
            addition function with a shared variable. The program has capabilities of 
            displaying the effects and existence of race conditions, and also explores 
            scheduling and synchronization mechanisms to solve these race conditions. 
    
    - lab2_add.csv: 
            This contains the output of all the tests in the Makefile that use lab2_add.c 
            to display the existence of race conditions, as well as the ability to use
            synchronization methods to stop race conditions from occuring. 
    
    - lab2_add-1.png:
            A plot/chart that displays the number of threads and iterations required to 
            generate a failure (with and without yields).

    - lab2_add-2.png:
            A plot/chart that displays average time per operation with and without yields.

    - lab2_add-3.png:
            A plot/chart that displays the average time per (single threaded) operation 
            vs. the number of iterations.

    - lab2_add-4.png:
            A plot/chart that displays number of threads and iterations that can run successfully 
            with yields under each of the synchronization options.

    - lab2_add-5.png: 
            A plot/chart that displays the average time per (protected) operation vs. the number of 
            threads.

    - lab2_list.c:
            This is a C program that tests a shared sorted linked list. It has capabilities
            to multithread to show the existence of race conditions, as well as the 
            ability to demonstrate scheduling and synchronization mechanisms to get rid 
            of any existing race conditions.

    - lab2_list.csv
            This contains the output of all the tests in the Makefile that use lab2_list.c 
            to display the existence of race conditions, as well as the ability to use 
            synchronization methods to stop race conditions from occuring. 
    
    - lab2_list-1.png:
            A plot/chart that displays the average time per (single threaded) unprotected 
            operation vs. number of iterations (illustrating the correction of the per-operation 
            cost for the list length).

    - lab2_list-2.png:
            A plot/chart that displays the number of threads and iterations required to 
            generate a failure (with and without yields).
            
    - lab2_list-3.png: 
            A plot/chart that displays the number of iterations that can run (protected) without 
            failure.
            
    - lab2_list-4.png:
            A plot/chart that displays the length-adjusted cost per operation vs the number of 
            threads for the various synchronization options.

    - SortedList.h && SortedList.c
            Together, they contain function definitions and function implementations to 
            insert, delete or search for an element in a doubly linked list, as well as 
            calculate the length of sorted linked list.

    - lab2_add.gp:
            This is a graphing module that uses gnuplot to create 5 different graphs for 
            output in lab2_add.csv, namely, lab2_add-1.png, lab2_add-2.png, lab2_add-3.png, 
            lab2_add-4.png and lab2_add-5.png that contain different charts about 
            usage/timing statistics.
    
    - lab2_list.gp:
            This is a graphing module that uses gnuplot to create 5 different graphs for 
            output in lab2_list.csv, namely, lab2_list-1.png, lab2_list-2.png, lab2_list-3.png, 
            and  lab2_list-4.png that contain different charts about usage/timing statistics.

    - README:
            The README file contains a description of the all the files in the compressed
            tarball, as well as answers to questions for Part-1 and Part-2 of Lab2A. 
    
    - Makefile:
            The Makefile supports build, tests, graphs, clean and dist. Build acts like 
            make default, tests run tests for lab2_add.c and lab2_list.c, graphs takes 
            the .csv files produced by tests and produces 9 different charts. Clean 
            removes all txt, object, png and .tar.gz files. Dist creates the compressed 
            tarball.

Questions:
    
    QUESTION 2.1.1 - causing conflicts:
        A. Why does it take many iterations before errors are seen?
        B. Why does a significantly smaller number of iterations so seldom fail?
    ANSWER 2.1.1:
        Part A and B of this question have the same answer (explained below).
        In the case of small number of iterations, by the time the next thread is 
        created, the previous thread is done running all the required iterations. 
        As the number of iterations increase, the thread takes significantly 
        longer, so the next thread and current thread access the same memory
        location parallely for a short overlapping period of time. As we increase
        the number of threads and number of iterations, more threads overlap with
        each other for longer durations/more iterations of the for loop, resulting
        in a more skewed/incorrect value for the counter. 

    
    QUESTION 2.1.2 - cost of yielding:
        A. Why are the --yield runs so much slower?
        B. Where is the additional time going?
        C. Is it possible to get valid per-operation timings if we are using the 
           --yield option?
        D. If so, explain how. If not, explain why not.
    ANSWER 2.1.2:
        Answers for part A and B:
            As mentioned in the linux man page for sched_yield(), the sched_yield
            function yields to the CPU/causes the thread to relinquish the CPU. 
            The result is that the current thread is moved to the end of the queue
            of threads that need to be executed, and thus forces the next thread 
            in the queue to run. The additional time goes into context switching,
            i.e., switching from one thread to the other thread. This is expensive,
            because it requires the CPU to save the current thread's local memory,
            current stack pointer, the program counter and registers. The CPU then 
            needs to switch to the next thread in the queue, loading its previous 
            state, memory, rsp, program counter and registers so it can execute it.
            The sched_yield() function is called after each addition in each thread,
            resulting in significant compounding of context-switch time. 
        Answers for part C and D:
            The clock_gettime() uses measures system-wide clock that measures real 
            (i.e., wall-clock) time (as seen in the documentation for clock_gettime() 
            when using the CLOCK_REALTIME clkid). It may be possible to estimate
            context switch time in the case of a single-threaded program if we were
            somehow able to determine the time for a single context-switch and also
            assume that the time of a context-switch is fairly consistent. However,
            in this case, multithreading results in multiple sched_yield() calls happening 
            in parallel and often causes these calls to overlap. Therefore, it becomes 
            nearly impossible to collect context-switch times/per-operation timings 
            accurately. 
    

    QUESTION 2.1.3 - measurement errors:
        A. Why does the average cost per operation drop with increasing iterations?
        B. If the cost per iteration is a function of the number of iterations, how 
           do we know how many iterations to run (or what the "correct" cost is)?
    ANSWER 2.1.3:
        Answer for A:
            As the data suggests, when the program is single-threaded, the average
            cost per operation (non-yielded) decreases significantly and eventually 
            becomes fairly constant beyond 1 million iterations. This is because the 
            cost of creating a single thread is fairly large. Therefore, if the number 
            of iterations are lower, it results in a larger cost per operation since it 
            contributes more significantly to the total runtime. This is a result of how 
            our code is structured, where clock_gettime() is called before the threads 
            are created and only stops after pthread_join is done executing. When the 
            number of iterations increase, the overall time increases signficantly, 
            reducing the weightage of the time/cost needed to create a single thread, 
            thereby reducing the average cost per operation. 
        Answer for B:
            Clearly, as the data suggests, the average cost per operation decreases
            exponentially with an increase in number of iterations. For all tests
            performed with iterations greater than 1 million, the average cost per 
            operation seems to be a converge towards a constant value. This suggests 
            a value around this ballpark is the "correct" cost. This is also true in 
            case of the graph seen in lab2_add-3.png. The curve decreases exponentially, 
            and the steepness of the curve decreases. It eventually becomes 
            flat/stable, and this suggests that this point can therefore be a reasonable 
            estimate of the "correct" cost.

    
    QUESTION 2.1.4 - costs of serialization:
        A. Why do all of the options perform similarly for low numbers of threads?
        B. Why do the three protected operations slow down as the number of threads 
            rises?
    ANSWER 2.1.4:
        ANSWER A:
            With a smaller number of threads, fewer number of locks/unlocks occur,
            resulting in a decrease in cases where multiple threads try to get into the 
            critical section. In other words, there are fewer instances where race 
            conditions occur. If fewer threads compete/wait to get into the 
            critical section, then only a small amount of additional time is 
            added to the total runtime for the instances where the threads actually
            have to wait. Therefore, they all perform similarly for low number of
            threads.
        ANSWER B:
            This is the inverse of the reason above. As the number of threads increase,
            there are more instances when threads compete to execute code in the critical
            section. This results in increased amount of waiting time per thread on 
            average and leads to each of the operations taking longer (and therefore, 
            slowing down as the number of threads increase).


    QUESTION 2.2.1 - scalability of Mutex
        A. Compare the variation in time per mutex-protected operation vs the number 
            of threads in Part-1 (adds) and Part-2 (sorted lists).
        B. Comment on the general shapes of the curves, and explain why they have this 
            shape.
        C. Comment on the relative rates of increase and differences in the shapes 
            of the curves, and offer an explanation for these differences.
    ANSWER 2.2.1:
        ANSWER A, B and C:
            The main difference in seen in the cost per mutex-protected operation.
            The data suggests that as the number of threads increase, the per 
            mutex-protected operation cost increases at a much higher/faster rate
            in Part-2 as compared to Part-1. While the number of operations done are 
            3 times as many in Part-2 as compared to Part-1, the per mutex-operation 
            cost is significantly higher for Part-2 because each list 
            operations in Part-2 is significantly more expensive than the add 
            operations in Part-1. 
            In Part-1, mutex protected operations have a decreasing straight line graph,
            representing a decreasing cost. This is because once the thread is inside
            the lock, it is able to complete all addition operations. Therefore, the 
            threads don't spend time competing. In Part-2, the shape of the mutex curve 
            is a positive upward line, representing an increase in cost with an 
            increase in the number of threads. This is because each list operation in 
            Part-2 is far more expensive than the addition operation in Part-1. 
            Additionally, each insertion increases the length, while each deletion 
            decreases the length thereby increasing cost linearly upwards. The rate of 
            increase in cost is higher in Part-2 than Part-1 because of the same 
            reason. On the whole, in both cases, the curve/line is linear as expected
            because of the behavior of mutex locks, where the threads rest when there is 
            a lock without continuosly using/consuming CPU cycles.


    QUESTION 2.2.2 - scalability of spin locks
        A. Compare the variation in time per protected operation vs the number of 
            threads for list operations protected by Mutex vs Spin locks. Comment on 
            the general shapes of the curves, and explain why they have this shape.
        B. Comment on the relative rates of increase and differences in the shapes of 
            the curves, and offer an explanation for these differences.
    ANSWER 2.2.2:
        ANSWER A AND B:
                Similarities:
                        In Part-2, the cost per protected operation increaseses 
                        with the increase in number of threads. As discussed in
                        Question 2.2.1, this is because more threads compete for
                        a single lock (mutex or spinlock), causing an increased 
                        overhead cost.
                Differences:
                        1. When there are fewer threads, spinlocks appear to have 
                        lower time per protected operation in comparison to 
                        mutex protected operations. However, the cost per 
                        operation for spinlocks increases/grows much larger than
                        that of mutex protected operations when there are a 
                        larger number of threads compete for the critical/locked 
                        section.
                        2. It is evident that the cost per operation for spin 
                        locks grows at a faster rate than that of mutex protected 
                        operations. 
                Both curves have positive slopes and represent lines, although the 
                plot from spin-lock seems to have a few troughs and peaks. 
                The reason for all of these differences lie in the internal mechanisms
                for how each of these mechanisms work. In the case of spinlocks, the 
                threads keeping running and utilizing CPU cycles to check if they can get
                access to the locked/critical section, resulting in an increased cost per
                operation due to this overhead. On the other hand, mutexes force the threads
                to hibernate/sleep when they are not running (when they don't have access to
                the critical/locked/protected sections). This prevents the overhead cost of 
                additional CPU cycles for constantly checking if the locked section is unlocked,
                resulting in lower cost per opearation. Therefore, for a larger number of 
                threads, spinlock would have significantly greater cost per operation since 
                its slope has a faster rate of change/increase as compared to that of the mutex
                curve/plot.